{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lucky-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/uc?export=download&id=1ZH3owx7KFZn36q0rEelKyn4PpUq_0H6H\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zipurl = 'https://drive.google.com/uc?export=download&id=1ZH3owx7KFZn36q0rEelKyn4PpUq_0H6H'\n",
    "    # Download the file from the URL\n",
    "zipresp = urlopen(zipurl)\n",
    "    # Create a new file on the hard drive\n",
    "tempzip = open(\"/tmp/tempfile.zip\", \"wb\")\n",
    "    # Write the contents of the downloaded file into the new file\n",
    "tempzip.write(zipresp.read())\n",
    "    # Close the newly-created file\n",
    "tempzip.close()\n",
    "    # Re-open the newly-created file with ZipFile()\n",
    "zf = ZipFile(\"/tmp/tempfile.zip\")\n",
    "    # Extract its contents into <extraction_path>\n",
    "    # note that extractall will automatically create the path\n",
    "zf.extractall(path = './')\n",
    "    # close the ZipFile instance\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from utils import add_special_tokens\n",
    "\n",
    "\n",
    "class GPT21024Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, ids_file, mode='train',length=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = add_special_tokens()\n",
    "\n",
    "        # with open(ids_file,'r') as f:\n",
    "            # if mode=='train':\n",
    "            #     self.idxs = np.array(json.load(f)['train_ids'])\n",
    "            # elif mode=='valid':\n",
    "            #     self.idxs = np.array(json.load(f)['valid_ids'])\n",
    "            # elif mode=='test':\n",
    "            #     self.idxs = np.array(json.load(f)['test_ids'])\n",
    "\n",
    "            # self.idxs = self.idxs -min(self.idxs)\n",
    "        \n",
    "        self.idxs = os.listdir(root_dir)\n",
    "        self.mode = mode\n",
    "        if len == None:\n",
    "            self.len = len(self.idxs)\n",
    "        else:\n",
    "            self.len = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        if self.mode=='valid':\n",
    "            idx = self.idxs[-idx]\n",
    "        elif self.mode=='test':\n",
    "            idx = self.idxs[-idx-self.len]   #assuming valid and test set of same sizes\n",
    "        else:\n",
    "            idx = self.idxs[idx]\n",
    "        # file_name = os.path.join(self.root_dir,str(idx)+\".json\")\n",
    "        file_name = os.path.join(self.root_dir,str(idx))\n",
    "        with open(file_name,'r') as f:\n",
    "              data = json.load(f)\n",
    "        text = self.tokenizer.encode(self.tokenizer.pad_token)*1024\n",
    "        content = data['article'] + self.tokenizer.encode(self.tokenizer.sep_token) + data['abstract']\n",
    "        text[:len(content)] = content\n",
    "        text = torch.tensor(text)\n",
    "        sample = {'article': text, 'sum_idx': len(data['article'])}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attached-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tnrange\n",
    "\n",
    "\n",
    "def add_special_tokens():\n",
    "\t\"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "\ttokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\tspecial_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "\tnum_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "\treturn tokenizer\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n",
    "    \"\"\" Generates a sequence of tokens \n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    generated = context\n",
    "    with torch.no_grad():  \n",
    "        for _ in tnrange(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated\n",
    "\n",
    "\n",
    "def beam_search(model, context, length, beam_size, device, temperature=1):\n",
    "    \"\"\" Generate sequence using beam search https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            beam_size: >=1 and <= total_no_of_tokens\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "    \"\"\"\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    with torch.no_grad():  \n",
    "        inputs = {'input_ids': context}\n",
    "        outputs = model(**inputs) \n",
    "        next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "        next_token_probs = F.softmax(next_token_logits)\n",
    "        scores, indices = torch.topk(next_token_probs, beam_size)\n",
    "        indices = indices.tolist()\n",
    "        sequences = [[c] for c in indices]\n",
    "        for _ in tnrange(length-1):\n",
    "            logits = torch.zeros(beam_size*len(next_token_logits))\n",
    "            for j in range(len(sequences)):\n",
    "                new_generated = torch.cat((context,torch.tensor([sequences[j]], dtype=torch.long, device=device)),dim=1)\n",
    "                inputs = {'input_ids': new_generated}\n",
    "                outputs = model(**inputs) \n",
    "                next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                next_token_probs = F.softmax(next_token_logits)\n",
    "                start, stop = j*len(next_token_logits), (j+1)*len(next_token_logits)\n",
    "                logits[start:stop] = scores[j]*next_token_probs\n",
    "            scores, new_logits_indices = torch.topk(logits,beam_size)\n",
    "            logits = (new_logits_indices%50259).tolist()\n",
    "            for j in range(len(sequences)):\n",
    "                sequences[j] = sequences[j]+[logits[j]]\n",
    "    return scores, sequences\n",
    "\n",
    "\n",
    "def generate_beam_sample(data, tokenizer, model, num=1, length=100, beam_size=3, device=torch.device('cuda')):\n",
    "    \"\"\" Generate summaries for \"num\" number of articles using beam search.\n",
    "        Args:\n",
    "            data = GPT21024Dataset object\n",
    "            tokenizer = gpt/gpt2 tokenizer\n",
    "            num = number of articles for which summaries has to be generated\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        sample = data[i]\n",
    "        idx = sample['sum_idx']\n",
    "        context = sample['article'][:idx].tolist()\n",
    "        summary = sample['article'][idx+1:][:100].tolist()\n",
    "        scores, sequences = beam_search(model, context, length, beam_size, device)\n",
    "        print('new_article', end='\\n\\n')\n",
    "        print(tokenizer.decode(context[:-1]), end='\\n\\n')\n",
    "        print('actual_summary', end='\\n\\n')\n",
    "        print(tokenizer.decode(summary), end='\\n\\n')\n",
    "        for i in range(len(sequences)):\n",
    "            text = tokenizer.convert_ids_to_tokens(sequences[i],skip_special_tokens=True)\n",
    "            text = tokenizer.convert_tokens_to_string(text)  \n",
    "            print(\"generated_summary-{} and Score is {}.\".format(i+1, scores[i]), end='\\n\\n')\n",
    "            print(text, end='\\n\\n')\n",
    "\n",
    "\n",
    "def generate_sample(data, tokenizer, model, num=1, eval_step=False, length=100, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n",
    "    \"\"\" Generate summaries for \"num\" number of articles.\n",
    "        Args:\n",
    "            data = GPT21024Dataset object\n",
    "            tokenizer = gpt/gpt2 tokenizer\n",
    "            model = gpt/gpt2 model\n",
    "            num = number of articles for which summaries has to be generated\n",
    "            eval_step = can be True/False, checks generating during evaluation or not\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        sample = data[i]\n",
    "        idx = sample['sum_idx']\n",
    "        context = sample['article'][:idx].tolist()\n",
    "        summary = sample['article'][idx+1:][:100].tolist()\n",
    "        generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n",
    "        generated_text = generated_text[0, len(context):].tolist()\n",
    "        text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "        text = tokenizer.convert_tokens_to_string(text)\n",
    "        if eval_step==False:\n",
    "            print('new_article', end='\\n\\n')\n",
    "            print(tokenizer.decode(context), end='\\n\\n')\n",
    "            print(\"generated_summary\", end='\\n\\n')\n",
    "            print(text, end='\\n\\n')\n",
    "            print('actual_summary', end='\\n\\n')\n",
    "            print(tokenizer.decode(summary), end='\\n\\n')\n",
    "        else:\n",
    "            print(tokenizer.decode(context), end='\\n\\n')\n",
    "            print(\"generated_summary\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "arbitrary-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from transformers import GPT2Config, GPT2LMHeadModel,AdamW, GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tnrange, tqdm\n",
    "\n",
    "\n",
    "def train(args, model, tokenizer, train_dataset, valid_dataset, ignore_index):\n",
    "    \"\"\" Trains GPT2 model and logs necessary details.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            tokenizer: GPT/GPT2 tokenizer\n",
    "            train_dataset: GPT21024Dataset object for training data\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter('./logs')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dl = DataLoader(train_dataset,sampler=train_sampler,batch_size=args.batch_size,num_workers=args.num_workers)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "    optimizer = AdamW(model.parameters(),lr=args.lr)\n",
    "    scheduler = scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps = 80000)\n",
    "    # WarmupLinearSchedule(optimizer,100,80000)\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = tnrange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dl, desc=\"Training\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = torch.tensor(batch['article']), torch.tensor(batch['article'])\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sum_idx'].item() # index of separator token\n",
    "            # only consider loss on reference summary just like seq2seq models\n",
    "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
    "            shift_labels = labels[..., idx+1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss = loss/args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n",
    "                logging_loss = tr_loss\n",
    "                print(\"loss:\", loss.item(), end='\\n\\n')\n",
    "                if (step + 1)/args.gradient_accumulation_steps == 1.0:\n",
    "                    print('After 1st update: ', end='\\n\\n')\n",
    "                    generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=False,device=args.device)\n",
    "\n",
    "\n",
    "            if (step + 1) % (10*args.gradient_accumulation_steps) == 0:\n",
    "                results = evaluate(args, model, valid_dataset, ignore_index, global_step)\n",
    "                for key, value in results.items():\n",
    "                    writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                print('After', global_step+1,'updates: ', end='\\n\\n')\n",
    "                generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=True,device=args.device)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(args, model, eval_dataset, ignore_index, global_step=None):\n",
    "    \"\"\" Returns perplexity score on validation dataset.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            eval_dataset: GPT21024Dataset object for validation data\n",
    "            global_step: no. of times gradients have backpropagated\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    results = {}\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = torch.tensor(batch['article']).to(args.device), torch.tensor(batch['article']).to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sum_idx'].item() # index of separator token\n",
    "            # only consider loss on reference summary just like seq2seq models\n",
    "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
    "            shift_labels = labels[..., idx+1:].contiguous()\n",
    "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    print(\"perplexity:\", perplexity.item())\n",
    "\n",
    "    if global_step:\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as f:\n",
    "            for key in sorted(result.keys()):\n",
    "                f.write('\\n\\n')\n",
    "                f.write(\"time = %s, %s = %s, step = %s\\n\" % (datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), key, str(result[key]), str(global_step)))\n",
    "    return result\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--lr\",default=5e-5, type=float, required=True, help=\"learning rate\")\n",
    "#     parser.add_argument(\"--seed\",default=42, type=int, required=False, help=\"seed to replicate results\")\n",
    "#     parser.add_argument(\"--n_gpu\",default=1, type=int, required=False, help=\"no of gpu available\")\n",
    "#     parser.add_argument(\"--gradient_accumulation_steps\",default=32, type=int, required=True, help=\"gradient_accumulation_steps\")\n",
    "#     parser.add_argument(\"--batch_size\",default=1, type=int, required=True, help=\"batch_size\")\n",
    "#     parser.add_argument(\"--num_workers\",default=4, type=int, required=False, help=\"num of cpus available\")\n",
    "#     parser.add_argument(\"--device\",default=torch.device('cpu'), required=False, help=\"torch.device object\")\n",
    "#     parser.add_argument(\"--num_train_epochs\",default=1, type=int, required=True, help=\"no of epochs of training\")\n",
    "#     parser.add_argument(\"--output_dir\",default='./output', type=str, required=True, help=\"path to save evaluation results\")\n",
    "#     parser.add_argument(\"--model_dir\",default='./weights', type=str, required=True, help=\"path to save trained model\")\n",
    "#     parser.add_argument(\"--fp16\",default=True, type=bool, required=False, help=\"whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "#     parser.add_argument(\"--fp16_opt_level\",default='O0', type=str, required=False, help=\"apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\")\n",
    "#     parser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "#     parser.add_argument(\"--root_dir\",default='./CNN/gpt2_1024_data', type=str, help=\"location of json dataset.\")\n",
    "#     parser.add_argument(\"--ids_file\",default='./CNN/ids.json', type=str, help=\"location of train, valid and test file indexes\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--lr\",default=5e-5, type=float, required=False, help=\"learning rate\")\n",
    "# parser.add_argument(\"--seed\",default=42, type=int, required=False, help=\"seed to replicate results\")\n",
    "# parser.add_argument(\"--n_gpu\",default=1, type=int, required=False, help=\"no of gpu available\")\n",
    "# parser.add_argument(\"--gradient_accumulation_steps\",default=32, type=int, required=False, help=\"gradient_accumulation_steps\")\n",
    "# parser.add_argument(\"--batch_size\",default=1, type=int, required=False, help=\"batch_size\")\n",
    "# parser.add_argument(\"--num_workers\",default=4, type=int, required=False, help=\"num of cpus available\")\n",
    "# parser.add_argument(\"--device\",default=torch.device('cpu'), required=False, help=\"torch.device object\")\n",
    "# parser.add_argument(\"--num_train_epochs\",default=1, type=int, required=False, help=\"no of epochs of training\")\n",
    "# parser.add_argument(\"--output_dir\",default='./output', type=str, required=False, help=\"path to save evaluation results\")\n",
    "# parser.add_argument(\"--model_dir\",default='./weights', type=str, required=False, help=\"path to save trained model\")\n",
    "# parser.add_argument(\"--fp16\",default=True, type=bool, required=False, help=\"whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "# parser.add_argument(\"--fp16_opt_level\",default='O0', type=str, required=False, help=\"apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\")\n",
    "# parser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "# parser.add_argument(\"--root_dir\",default='./CNN/gpt2_1024_data', type=str, help=\"location of json dataset.\")\n",
    "# parser.add_argument(\"--ids_file\",default='./CNN/ids.json', type=str, help=\"location of train, valid and test file indexes\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# train\n",
    "argsDict = { 'lr': 5e-5, 'seed': 42, 'n_gpu': 1, 'gradient_accumulation_steps': 32, 'batch_size': 1,\n",
    "        'num_workers': 4, 'device': torch.device('cpu'), 'num_train_epochs': 1, 'output_dir': 'output',\n",
    "        'model_dir': 'weights', 'fp16': True, 'fp16_opt_level': 'O0', 'max_grad_norm': 1.0,\n",
    "        'root_dir': 'CNN/gpt2_1024_data', 'ids_file': 'CNN/ids.json'\n",
    "       }\n",
    "args = SimpleNamespace(**argsDict)\n",
    "\n",
    "\n",
    "train_data = GPT21024Dataset('CNN/gpt2_1024_data','CNN/ids.json',mode='train',length=200) #training on only 3000 datasets\n",
    "valid_data = GPT21024Dataset('CNN/gpt2_1024_data','CNN/ids.json',mode='valid',length=50)  #validation on only 500 datasets\n",
    "tokenizer = add_special_tokens()\n",
    "ignore_idx = tokenizer.pad_token_id\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(args.device)\n",
    "\n",
    "start = time.time()\n",
    "train(args, model, tokenizer, train_data, valid_data, ignore_idx)\n",
    "print('total time: ', (time.time()-start)/60, \" minutes\", end='\\n\\n')\n",
    "\n",
    "print('Saving trained model...')\n",
    "model_file = os.path.join(args.model_dir, 'model_{}_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(args.fp16_opt_level,3000,args.num_train_epochs))\n",
    "config_file = os.path.join(args.model_dir, 'config_{}_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(args.fp16_opt_level,3000,args.num_train_epochs))\n",
    "torch.save(model.state_dict(), model_file)\n",
    "model.config.to_json_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-angel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
