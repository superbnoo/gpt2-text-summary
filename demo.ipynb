{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "executive-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/uc?export=download&id=1ZH3owx7KFZn36q0rEelKyn4PpUq_0H6H\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zipurl = 'https://drive.google.com/uc?export=download&id=1ZH3owx7KFZn36q0rEelKyn4PpUq_0H6H'\n",
    "    # Download the file from the URL\n",
    "zipresp = urlopen(zipurl)\n",
    "    # Create a new file on the hard drive\n",
    "tempzip = open(\"/tmp/tempfile.zip\", \"wb\")\n",
    "    # Write the contents of the downloaded file into the new file\n",
    "tempzip.write(zipresp.read())\n",
    "    # Close the newly-created file\n",
    "tempzip.close()\n",
    "    # Re-open the newly-created file with ZipFile()\n",
    "zf = ZipFile(\"/tmp/tempfile.zip\")\n",
    "    # Extract its contents into <extraction_path>\n",
    "    # note that extractall will automatically create the path\n",
    "zf.extractall(path = './')\n",
    "    # close the ZipFile instance\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title â € {display-mode: \"form\"}\n",
    "\n",
    "def stylize():\n",
    "    \"Handle dark mode\"\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "    :root {\n",
    "        --table_bg: #EBF8FF;\n",
    "    }\n",
    "    html[theme=dark] {\n",
    "        --colab-primary-text-color: #d5d5d5;\n",
    "        --table_bg: #2A4365;\n",
    "    }\n",
    "    .jupyter-widgets {\n",
    "        color: var(--colab-primary-text-color);\n",
    "    }\n",
    "    table {\n",
    "        border-collapse: collapse !important;\n",
    "    }\n",
    "    td {\n",
    "        text-align:left !important;\n",
    "        border: solid var(--table_bg) !important;\n",
    "        border-width: 1px 0 !important;\n",
    "        padding: 6px !important;\n",
    "    }\n",
    "    tr:nth-child(even) {\n",
    "        background-color: var(--table_bg) !important;\n",
    "    }\n",
    "    .table_odd {\n",
    "        background-color: var(--table_bg) !important;\n",
    "        margin: 0 !important;\n",
    "    }\n",
    "    .table_even {\n",
    "        border: solid var(--table_bg) !important;\n",
    "        border-width: 1px 0 !important;\n",
    "        margin: 0 !important;\n",
    "    }\n",
    "    .jupyter-widgets {\n",
    "        margin: 6px;\n",
    "    }\n",
    "    .widget-html-content {\n",
    "        font-size: var(--colab-chrome-font-size) !important;\n",
    "        line-height: 1.24 !important;\n",
    "    }\n",
    "    </style>'''))\n",
    "\n",
    "def print_html(x):\n",
    "    \"Better printing\"\n",
    "    x = x.replace('\\n', '<br>')\n",
    "    display(HTML(x))\n",
    "        \n",
    "# Check we use GPU\n",
    "import torch\n",
    "from IPython.display import display, HTML, Javascript, clear_output\n",
    "if not torch.cuda.is_available():\n",
    "    print_html('Error: GPU was not found\\n1/ click on the \"Runtime\" menu and \"Change runtime type\"\\n'\\\n",
    "          '2/ set \"Hardware accelerator\" to \"GPU\" and click \"save\"\\n3/ click on the \"Runtime\" menu, then \"Run all\" (below error should disappear)')\n",
    "    raise ValueError('No GPU available')\n",
    "else:\n",
    "    # colab requires special handling\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "\n",
    "    # Install dependencies (mainly for colab)\n",
    "    if IN_COLAB:\n",
    "        !pip install transformers\n",
    "        !pip install torch wandb -qq\n",
    "        !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "        !sudo apt-get install git-lfs\n",
    "\n",
    "    # dataset\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from torch.utils.data import Dataset\n",
    "    from transformers import GPT2Tokenizer\n",
    "    import pickle\n",
    "    import random\n",
    "    import sys\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from tqdm import tnrange, tqdm\n",
    "    from types import SimpleNamespace\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    from transformers import GPT2Config, GPT2LMHeadModel,AdamW, GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "    \n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "    \n",
    "\n",
    "    def add_special_tokens():\n",
    "        \"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "        num_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "        return tokenizer\n",
    "\n",
    "    def set_seed(args):\n",
    "        random.seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        if args.n_gpu > 0:\n",
    "            torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "    def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "        \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "            Args:\n",
    "                logits: logits distribution shape (vocabulary size)\n",
    "                top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "                top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                    Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "            From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "        \"\"\"\n",
    "        assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "        top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "        if top_k > 0:\n",
    "            # Remove all tokens with a probability less than the last token of the top-k\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "\n",
    "        if top_p > 0.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the indices to the right to keep also the first token above the threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n",
    "        \"\"\" Generates a sequence of tokens \n",
    "            Args:\n",
    "                model: gpt/gpt2 model\n",
    "                context: tokenized text using gpt/gpt2 tokenizer\n",
    "                length: length of generated sequence.\n",
    "                device: torch.device object.\n",
    "                temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "                top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "                top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "        \"\"\"\n",
    "\n",
    "        context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "        context = context.unsqueeze(0)\n",
    "        generated = context\n",
    "        with torch.no_grad():  \n",
    "            for _ in tnrange(length):\n",
    "                inputs = {'input_ids': generated}\n",
    "                outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "                next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "        return generated\n",
    "\n",
    "\n",
    "    def beam_search(model, context, length, beam_size, device, temperature=1):\n",
    "        \"\"\" Generate sequence using beam search https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "            Args:\n",
    "                model: gpt/gpt2 model\n",
    "                context: tokenized text using gpt/gpt2 tokenizer\n",
    "                length: length of generated sequence.\n",
    "                beam_size: >=1 and <= total_no_of_tokens\n",
    "                device: torch.device object.\n",
    "                temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "        \"\"\"\n",
    "        context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "        context = context.unsqueeze(0)\n",
    "        with torch.no_grad():  \n",
    "            inputs = {'input_ids': context}\n",
    "            outputs = model(**inputs) \n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            next_token_probs = F.softmax(next_token_logits)\n",
    "            scores, indices = torch.topk(next_token_probs, beam_size)\n",
    "            indices = indices.tolist()\n",
    "            sequences = [[c] for c in indices]\n",
    "            for _ in tnrange(length-1):\n",
    "                logits = torch.zeros(beam_size*len(next_token_logits))\n",
    "                for j in range(len(sequences)):\n",
    "                    new_generated = torch.cat((context,torch.tensor([sequences[j]], dtype=torch.long, device=device)),dim=1)\n",
    "                    inputs = {'input_ids': new_generated}\n",
    "                    outputs = model(**inputs) \n",
    "                    next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                    next_token_probs = F.softmax(next_token_logits)\n",
    "                    start, stop = j*len(next_token_logits), (j+1)*len(next_token_logits)\n",
    "                    logits[start:stop] = scores[j]*next_token_probs\n",
    "                scores, new_logits_indices = torch.topk(logits,beam_size)\n",
    "                logits = (new_logits_indices%50259).tolist()\n",
    "                for j in range(len(sequences)):\n",
    "                    sequences[j] = sequences[j]+[logits[j]]\n",
    "        return scores, sequences\n",
    "\n",
    "\n",
    "    def generate_beam_sample(data, tokenizer, model, num=1, length=100, beam_size=3, device=torch.device('cuda')):\n",
    "        \"\"\" Generate summaries for \"num\" number of articles using beam search.\n",
    "            Args:\n",
    "                data = GPT21024Dataset object\n",
    "                tokenizer = gpt/gpt2 tokenizer\n",
    "                num = number of articles for which summaries has to be generated\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sample = data[i]\n",
    "            idx = sample['sum_idx']\n",
    "            context = sample['article'][:idx].tolist()\n",
    "            summary = sample['article'][idx+1:][:100].tolist()\n",
    "            scores, sequences = beam_search(model, context, length, beam_size, device)\n",
    "            print('new_article', end='\\n\\n')\n",
    "            print(tokenizer.decode(context[:-1]), end='\\n\\n')\n",
    "            print('actual_summary', end='\\n\\n')\n",
    "            print(tokenizer.decode(summary), end='\\n\\n')\n",
    "            for i in range(len(sequences)):\n",
    "                text = tokenizer.convert_ids_to_tokens(sequences[i],skip_special_tokens=True)\n",
    "                text = tokenizer.convert_tokens_to_string(text)  \n",
    "                print(\"generated_summary-{} and Score is {}.\".format(i+1, scores[i]), end='\\n\\n')\n",
    "                print(text, end='\\n\\n')\n",
    "\n",
    "\n",
    "    def generate_sample(data, tokenizer, model, num=1, eval_step=False, length=100, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n",
    "        \"\"\" Generate summaries for \"num\" number of articles.\n",
    "            Args:\n",
    "                data = GPT21024Dataset object\n",
    "                tokenizer = gpt/gpt2 tokenizer\n",
    "                model = gpt/gpt2 model\n",
    "                num = number of articles for which summaries has to be generated\n",
    "                eval_step = can be True/False, checks generating during evaluation or not\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sample = data[i]\n",
    "            idx = sample['sum_idx']\n",
    "            context = sample['article'][:idx].tolist()\n",
    "            summary = sample['article'][idx+1:][:100].tolist()\n",
    "            generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n",
    "            generated_text = generated_text[0, len(context):].tolist()\n",
    "            text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "            text = tokenizer.convert_tokens_to_string(text)\n",
    "            if eval_step==False:\n",
    "                print('new_article', end='\\n\\n')\n",
    "                print(tokenizer.decode(context), end='\\n\\n')\n",
    "                print(\"generated_summary\", end='\\n\\n')\n",
    "                print(text, end='\\n\\n')\n",
    "                print('actual_summary', end='\\n\\n')\n",
    "                print(tokenizer.decode(summary), end='\\n\\n')\n",
    "            else:\n",
    "                print(tokenizer.decode(context), end='\\n\\n')\n",
    "                print(\"generated_summary\", end='\\n\\n')\n",
    "\n",
    "    # dataset\n",
    "    class GPT21024Dataset(Dataset):\n",
    "\n",
    "        def __init__(self, root_dir, ids_file, mode='train',length=None):\n",
    "            self.root_dir = root_dir\n",
    "            self.tokenizer = add_special_tokens()\n",
    "\n",
    "            # with open(ids_file,'r') as f:\n",
    "                # if mode=='train':\n",
    "                #     self.idxs = np.array(json.load(f)['train_ids'])\n",
    "                # elif mode=='valid':\n",
    "                #     self.idxs = np.array(json.load(f)['valid_ids'])\n",
    "                # elif mode=='test':\n",
    "                #     self.idxs = np.array(json.load(f)['test_ids'])\n",
    "\n",
    "                # self.idxs = self.idxs -min(self.idxs)\n",
    "\n",
    "            self.idxs = os.listdir(root_dir)\n",
    "            self.mode = mode\n",
    "            if len == None:\n",
    "                self.len = len(self.idxs)\n",
    "            else:\n",
    "                self.len = length\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "        def __getitem__(self,idx):\n",
    "\n",
    "            if self.mode=='valid':\n",
    "                idx = self.idxs[-idx]\n",
    "            elif self.mode=='test':\n",
    "                idx = self.idxs[-idx-self.len]   #assuming valid and test set of same sizes\n",
    "            else:\n",
    "                idx = self.idxs[idx]\n",
    "            # file_name = os.path.join(self.root_dir,str(idx)+\".json\")\n",
    "            file_name = os.path.join(self.root_dir,str(idx))\n",
    "            with open(file_name,'r') as f:\n",
    "                  data = json.load(f)\n",
    "            text = self.tokenizer.encode(self.tokenizer.pad_token)*1024\n",
    "            content = data['article'] + self.tokenizer.encode(self.tokenizer.sep_token) + data['abstract']\n",
    "            text[:len(content)] = content\n",
    "            text = torch.tensor(text)\n",
    "            sample = {'article': text, 'sum_idx': len(data['article'])}\n",
    "            return sample\n",
    "        \n",
    "    def train(args, model, tokenizer, train_dataset, valid_dataset, ignore_index):\n",
    "        \"\"\" Trains GPT2 model and logs necessary details.\n",
    "            Args:\n",
    "                args: dict that contains all the necessary information passed by user while training\n",
    "                model: finetuned gpt/gpt2 model\n",
    "                tokenizer: GPT/GPT2 tokenizer\n",
    "                train_dataset: GPT21024Dataset object for training data\n",
    "                ignore_index: token not considered in loss calculation\n",
    "        \"\"\"\n",
    "        # writer = SummaryWriter('./logs')\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        train_dl = DataLoader(train_dataset,sampler=train_sampler,batch_size=args.batch_size,num_workers=args.num_workers)\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "        optimizer = AdamW(model.parameters(),lr=args.lr)\n",
    "        scheduler = scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps = 80000)\n",
    "        # WarmupLinearSchedule(optimizer,100,80000)\n",
    "\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        model.zero_grad()\n",
    "        train_iterator = tnrange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        set_seed(args)\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dl, desc=\"Training\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                inputs, labels = torch.tensor(batch['article']), torch.tensor(batch['article'])\n",
    "                inputs = inputs.to(args.device)\n",
    "                labels = labels.to(args.device)\n",
    "                model.train()\n",
    "                logits = model(inputs)[0]\n",
    "                idx = batch['sum_idx'].item() # index of separator token\n",
    "                # only consider loss on reference summary just like seq2seq models\n",
    "                shift_logits = logits[..., idx:-1, :].contiguous()\n",
    "                shift_labels = labels[..., idx+1:].contiguous()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                loss = loss/args.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "                    # writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    # writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "                    print(\"loss:\", loss.item(), end='\\n\\n')\n",
    "                    if (step + 1)/args.gradient_accumulation_steps == 1.0:\n",
    "                        print('After 1st update: ', end='\\n\\n')\n",
    "                        generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=False,device=args.device)\n",
    "\n",
    "\n",
    "                if (step + 1) % (10*args.gradient_accumulation_steps) == 0:\n",
    "                    results = evaluate(args, model, valid_dataset, ignore_index, global_step)\n",
    "                    # for key, value in results.items():\n",
    "                    #     writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                    print('After', global_step+1,'updates: ', end='\\n\\n')\n",
    "                    generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=True,device=args.device)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(args, model, eval_dataset, ignore_index, global_step=None):\n",
    "        \"\"\" Returns perplexity score on validation dataset.\n",
    "            Args:\n",
    "                args: dict that contains all the necessary information passed by user while training\n",
    "                model: finetuned gpt/gpt2 model\n",
    "                eval_dataset: GPT21024Dataset object for validation data\n",
    "                global_step: no. of times gradients have backpropagated\n",
    "                ignore_index: token not considered in loss calculation\n",
    "        \"\"\"\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.mkdir(args.output_dir)\n",
    "        eval_output_dir = args.output_dir\n",
    "\n",
    "        results = {}\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size)\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = torch.tensor(batch['article']).to(args.device), torch.tensor(batch['article']).to(args.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(inputs)[0]\n",
    "                idx = batch['sum_idx'].item() # index of separator token\n",
    "                # only consider loss on reference summary just like seq2seq models\n",
    "                shift_logits = logits[..., idx:-1, :].contiguous()\n",
    "                shift_labels = labels[..., idx+1:].contiguous()\n",
    "                lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                eval_loss += lm_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "        result = {\n",
    "            \"perplexity\": perplexity\n",
    "        }\n",
    "        print(\"perplexity:\", perplexity.item())\n",
    "\n",
    "        if global_step:\n",
    "            output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "            with open(output_eval_file, \"a\") as f:\n",
    "                for key in sorted(result.keys()):\n",
    "                    f.write('\\n\\n')\n",
    "                    f.write(\"time = %s, %s = %s, step = %s\\n\" % (datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), key, str(result[key]), str(global_step)))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "argsDict = { 'lr': 5e-5, 'seed': 42, 'n_gpu': 1, 'gradient_accumulation_steps': 32, 'batch_size': 1,\n",
    "        'num_workers': 4, 'device': torch.device('cuda'), 'num_train_epochs': 1, 'output_dir': 'output',\n",
    "        'model_dir': 'weights', 'fp16': True, 'fp16_opt_level': 'O0', 'max_grad_norm': 1.0,\n",
    "        'root_dir': 'CNN/gpt2_1024_data', 'ids_file': 'CNN/ids.json'\n",
    "       }\n",
    "args = SimpleNamespace(**argsDict)\n",
    "\n",
    "\n",
    "train_data = GPT21024Dataset('CNN/gpt2_1024_data','CNN/ids.json',mode='train',length=3000) #training on only 3000 datasets\n",
    "valid_data = GPT21024Dataset('CNN/gpt2_1024_data','CNN/ids.json',mode='valid',length=500)  #validation on only 500 datasets\n",
    "tokenizer = add_special_tokens()\n",
    "ignore_idx = tokenizer.pad_token_id\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(args.device)\n",
    "\n",
    "start = time.time()\n",
    "train(args, model, tokenizer, train_data, valid_data, ignore_idx)\n",
    "print('total time: ', (time.time()-start)/60, \" minutes\", end='\\n\\n')\n",
    "\n",
    "print('Saving trained model...')\n",
    "model_file = os.path.join('model_{}_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(args.fp16_opt_level,3000,args.num_train_epochs))\n",
    "config_file = os.path.join('config_{}_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(args.fp16_opt_level,3000,args.num_train_epochs))\n",
    "torch.save(model.state_dict(), model_file)\n",
    "model.config.to_json_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-listing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
