{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "manufactured-westminster",
   "metadata": {},
   "source": [
    "# 1. load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/uc?export=download&id=1ZH3owx7KFZn36q0rEelKyn4PpUq_0H6H\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zipurl = 'https://drive.google.com/uc?export=download&id=1ZH3owx7KFZn36q0rEelKyn4PpUq_0H6H'\n",
    "    # Download the file from the URL\n",
    "zipresp = urlopen(zipurl)\n",
    "    # Create a new file on the hard drive\n",
    "tempzip = open(\"/tmp/tempfile.zip\", \"wb\")\n",
    "    # Write the contents of the downloaded file into the new file\n",
    "tempzip.write(zipresp.read())\n",
    "    # Close the newly-created file\n",
    "tempzip.close()\n",
    "    # Re-open the newly-created file with ZipFile()\n",
    "zf = ZipFile(\"/tmp/tempfile.zip\")\n",
    "    # Extract its contents into <extraction_path>\n",
    "    # note that extractall will automatically create the path\n",
    "zf.extractall(path = './')\n",
    "    # close the ZipFile instance\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-subsection",
   "metadata": {},
   "source": [
    "# 2. all the source code is bundled to the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "based-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title â € {display-mode: \"form\"}\n",
    "\n",
    "def stylize():\n",
    "    \"Handle dark mode\"\n",
    "    display(HTML('''\n",
    "    <style>\n",
    "    :root {\n",
    "        --table_bg: #EBF8FF;\n",
    "    }\n",
    "    html[theme=dark] {\n",
    "        --colab-primary-text-color: #d5d5d5;\n",
    "        --table_bg: #2A4365;\n",
    "    }\n",
    "    .jupyter-widgets {\n",
    "        color: var(--colab-primary-text-color);\n",
    "    }\n",
    "    table {\n",
    "        border-collapse: collapse !important;\n",
    "    }\n",
    "    td {\n",
    "        text-align:left !important;\n",
    "        border: solid var(--table_bg) !important;\n",
    "        border-width: 1px 0 !important;\n",
    "        padding: 6px !important;\n",
    "    }\n",
    "    tr:nth-child(even) {\n",
    "        background-color: var(--table_bg) !important;\n",
    "    }\n",
    "    .table_odd {\n",
    "        background-color: var(--table_bg) !important;\n",
    "        margin: 0 !important;\n",
    "    }\n",
    "    .table_even {\n",
    "        border: solid var(--table_bg) !important;\n",
    "        border-width: 1px 0 !important;\n",
    "        margin: 0 !important;\n",
    "    }\n",
    "    .jupyter-widgets {\n",
    "        margin: 6px;\n",
    "    }\n",
    "    .widget-html-content {\n",
    "        font-size: var(--colab-chrome-font-size) !important;\n",
    "        line-height: 1.24 !important;\n",
    "    }\n",
    "    </style>'''))\n",
    "\n",
    "def print_html(x):\n",
    "    \"Better printing\"\n",
    "    x = x.replace('\\n', '<br>')\n",
    "    display(HTML(x))\n",
    "        \n",
    "# Check we use GPU\n",
    "import torch\n",
    "from IPython.display import display, HTML, Javascript, clear_output\n",
    "\n",
    "# quick check bypass\n",
    "# if not torch.cuda.is_available(): # cuda\n",
    "# if torch.cuda.is_available(): # cpu\n",
    "if not torch.cuda.is_available():\n",
    "    print_html('Error: GPU was not found\\n1/ click on the \"Runtime\" menu and \"Change runtime type\"\\n'\\\n",
    "          '2/ set \"Hardware accelerator\" to \"GPU\" and click \"save\"\\n3/ click on the \"Runtime\" menu, then \"Run all\" (below error should disappear)')\n",
    "    raise ValueError('No GPU available')\n",
    "else:\n",
    "    # colab requires special handling\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "\n",
    "    # Install dependencies (mainly for colab)\n",
    "    if IN_COLAB:\n",
    "        !pip install transformers\n",
    "        !pip install torch wandb -qq\n",
    "        !curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "        !sudo apt-get install git-lfs\n",
    "\n",
    "    # dataset\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from torch.utils.data import Dataset\n",
    "    from transformers import GPT2Tokenizer\n",
    "    import pickle\n",
    "    import random\n",
    "    import sys\n",
    "    import time\n",
    "    import torch.nn.functional as F\n",
    "    from tqdm import tnrange, tqdm\n",
    "    from types import SimpleNamespace\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    from transformers import GPT2Config, GPT2LMHeadModel,AdamW, GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "    \n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "    \n",
    "\n",
    "    def add_special_tokens():\n",
    "        \"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "        num_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "        return tokenizer\n",
    "\n",
    "    def set_seed(args):\n",
    "        random.seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        if args.n_gpu > 0:\n",
    "            torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "    def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "        \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "            Args:\n",
    "                logits: logits distribution shape (vocabulary size)\n",
    "                top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "                top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                    Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "            From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "        \"\"\"\n",
    "        assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "        top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "        if top_k > 0:\n",
    "            # Remove all tokens with a probability less than the last token of the top-k\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "\n",
    "        if top_p > 0.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            # Shift the indices to the right to keep also the first token above the threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n",
    "        \"\"\" Generates a sequence of tokens \n",
    "            Args:\n",
    "                model: gpt/gpt2 model\n",
    "                context: tokenized text using gpt/gpt2 tokenizer\n",
    "                length: length of generated sequence.\n",
    "                device: torch.device object.\n",
    "                temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "                top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "                top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "        \"\"\"\n",
    "\n",
    "        context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "        context = context.unsqueeze(0)\n",
    "        generated = context\n",
    "        with torch.no_grad():  \n",
    "            for _ in tnrange(length):\n",
    "                inputs = {'input_ids': generated}\n",
    "                outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "                next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "        return generated\n",
    "\n",
    "\n",
    "    def beam_search(model, context, length, beam_size, device, temperature=1):\n",
    "        \"\"\" Generate sequence using beam search https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "            Args:\n",
    "                model: gpt/gpt2 model\n",
    "                context: tokenized text using gpt/gpt2 tokenizer\n",
    "                length: length of generated sequence.\n",
    "                beam_size: >=1 and <= total_no_of_tokens\n",
    "                device: torch.device object.\n",
    "                temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "        \"\"\"\n",
    "        context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "        context = context.unsqueeze(0)\n",
    "        with torch.no_grad():  \n",
    "            inputs = {'input_ids': context}\n",
    "            outputs = model(**inputs) \n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            next_token_probs = F.softmax(next_token_logits)\n",
    "            scores, indices = torch.topk(next_token_probs, beam_size)\n",
    "            indices = indices.tolist()\n",
    "            sequences = [[c] for c in indices]\n",
    "            for _ in tnrange(length-1):\n",
    "                logits = torch.zeros(beam_size*len(next_token_logits))\n",
    "                for j in range(len(sequences)):\n",
    "                    new_generated = torch.cat((context,torch.tensor([sequences[j]], dtype=torch.long, device=device)),dim=1)\n",
    "                    inputs = {'input_ids': new_generated}\n",
    "                    outputs = model(**inputs) \n",
    "                    next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                    next_token_probs = F.softmax(next_token_logits)\n",
    "                    start, stop = j*len(next_token_logits), (j+1)*len(next_token_logits)\n",
    "                    logits[start:stop] = scores[j]*next_token_probs\n",
    "                scores, new_logits_indices = torch.topk(logits,beam_size)\n",
    "                logits = (new_logits_indices%50259).tolist()\n",
    "                for j in range(len(sequences)):\n",
    "                    sequences[j] = sequences[j]+[logits[j]]\n",
    "        return scores, sequences\n",
    "\n",
    "\n",
    "    def generate_beam_sample(data, tokenizer, model, num=1, length=100, beam_size=3, device=torch.device('cuda')):\n",
    "        \"\"\" Generate summaries for \"num\" number of articles using beam search.\n",
    "            Args:\n",
    "                data = GPT21024Dataset object\n",
    "                tokenizer = gpt/gpt2 tokenizer\n",
    "                num = number of articles for which summaries has to be generated\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sample = data[i]\n",
    "            idx = sample['sum_idx']\n",
    "            context = sample['article'][:idx].tolist()\n",
    "            summary = sample['article'][idx+1:][:100].tolist()\n",
    "            scores, sequences = beam_search(model, context, length, beam_size, device)\n",
    "            print('new_article', end='\\n\\n')\n",
    "            print(tokenizer.decode(context[:-1]), end='\\n\\n')\n",
    "            print('actual_summary', end='\\n\\n')\n",
    "            print(tokenizer.decode(summary), end='\\n\\n')\n",
    "            for i in range(len(sequences)):\n",
    "                text = tokenizer.convert_ids_to_tokens(sequences[i],skip_special_tokens=True)\n",
    "                text = tokenizer.convert_tokens_to_string(text)  \n",
    "                print(\"generated_summary-{} and Score is {}.\".format(i+1, scores[i]), end='\\n\\n')\n",
    "                print(text, end='\\n\\n')\n",
    "\n",
    "\n",
    "    def generate_sample(data, tokenizer, model, num=1, eval_step=False, length=100, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n",
    "        \"\"\" Generate summaries for \"num\" number of articles.\n",
    "            Args:\n",
    "                data = GPT21024Dataset object\n",
    "                tokenizer = gpt/gpt2 tokenizer\n",
    "                model = gpt/gpt2 model\n",
    "                num = number of articles for which summaries has to be generated\n",
    "                eval_step = can be True/False, checks generating during evaluation or not\n",
    "        \"\"\"\n",
    "        for i in range(num):\n",
    "            sample = data[i]\n",
    "            idx = sample['sum_idx']\n",
    "            context = sample['article'][:idx].tolist()\n",
    "            summary = sample['article'][idx+1:][:100].tolist()\n",
    "            generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n",
    "            generated_text = generated_text[0, len(context):].tolist()\n",
    "            text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "            text = tokenizer.convert_tokens_to_string(text)\n",
    "            if eval_step==False:\n",
    "                print('new_article', end='\\n\\n')\n",
    "                print(tokenizer.decode(context), end='\\n\\n')\n",
    "                print(\"generated_summary\", end='\\n\\n')\n",
    "                print(text, end='\\n\\n')\n",
    "                print('actual_summary', end='\\n\\n')\n",
    "                print(tokenizer.decode(summary), end='\\n\\n')\n",
    "            else:\n",
    "                print(tokenizer.decode(context), end='\\n\\n')\n",
    "                print(\"generated_summary\", end='\\n\\n')\n",
    "\n",
    "    # dataset\n",
    "    class GPT21024Dataset(Dataset):\n",
    "\n",
    "        def __init__(self, root_dir, ids_file, mode='train',length=None):\n",
    "            self.root_dir = root_dir\n",
    "            self.tokenizer = add_special_tokens()\n",
    "\n",
    "            # with open(ids_file,'r') as f:\n",
    "                # if mode=='train':\n",
    "                #     self.idxs = np.array(json.load(f)['train_ids'])\n",
    "                # elif mode=='valid':\n",
    "                #     self.idxs = np.array(json.load(f)['valid_ids'])\n",
    "                # elif mode=='test':\n",
    "                #     self.idxs = np.array(json.load(f)['test_ids'])\n",
    "\n",
    "                # self.idxs = self.idxs -min(self.idxs)\n",
    "\n",
    "            self.idxs = os.listdir(root_dir)\n",
    "            self.mode = mode\n",
    "            if len == None:\n",
    "                self.len = len(self.idxs)\n",
    "            else:\n",
    "                self.len = length\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "        def __getitem__(self,idx):\n",
    "\n",
    "            if self.mode=='valid':\n",
    "                idx = self.idxs[-idx]\n",
    "            elif self.mode=='test':\n",
    "                idx = self.idxs[-idx-self.len]   #assuming valid and test set of same sizes\n",
    "            else:\n",
    "                idx = self.idxs[idx]\n",
    "            # file_name = os.path.join(self.root_dir,str(idx)+\".json\")\n",
    "            file_name = os.path.join(self.root_dir,str(idx))\n",
    "            with open(file_name,'r') as f:\n",
    "                  data = json.load(f)\n",
    "            text = self.tokenizer.encode(self.tokenizer.pad_token)*1024\n",
    "            content = data['article'] + self.tokenizer.encode(self.tokenizer.sep_token) + data['abstract']\n",
    "            text[:len(content)] = content\n",
    "            text = torch.tensor(text)\n",
    "            sample = {'article': text, 'sum_idx': len(data['article'])}\n",
    "            return sample\n",
    "        \n",
    "    def train(args, model, tokenizer, train_dataset, valid_dataset, ignore_index):\n",
    "        \"\"\" Trains GPT2 model and logs necessary details.\n",
    "            Args:\n",
    "                args: dict that contains all the necessary information passed by user while training\n",
    "                model: finetuned gpt/gpt2 model\n",
    "                tokenizer: GPT/GPT2 tokenizer\n",
    "                train_dataset: GPT21024Dataset object for training data\n",
    "                ignore_index: token not considered in loss calculation\n",
    "        \"\"\"\n",
    "        # writer = SummaryWriter('./logs')\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        train_dl = DataLoader(train_dataset,sampler=train_sampler,batch_size=args.batch_size,num_workers=args.num_workers)\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "        optimizer = AdamW(model.parameters(),lr=args.lr)\n",
    "        scheduler = scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps = 80000)\n",
    "        # WarmupLinearSchedule(optimizer,100,80000)\n",
    "\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        model.zero_grad()\n",
    "        train_iterator = tnrange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "        set_seed(args)\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dl, desc=\"Training\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                inputs, labels = torch.tensor(batch['article']), torch.tensor(batch['article'])\n",
    "                inputs = inputs.to(args.device)\n",
    "                labels = labels.to(args.device)\n",
    "                model.train()\n",
    "                logits = model(inputs)[0]\n",
    "                idx = batch['sum_idx'].item() # index of separator token\n",
    "                # only consider loss on reference summary just like seq2seq models\n",
    "                shift_logits = logits[..., idx:-1, :].contiguous()\n",
    "                shift_labels = labels[..., idx+1:].contiguous()\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                loss = loss/args.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "                    # writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    # writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "                    print(\"loss:\", loss.item(), end='\\n\\n')\n",
    "                    if (step + 1)/args.gradient_accumulation_steps == 1.0:\n",
    "                        print('After 1st update: ', end='\\n\\n')\n",
    "                        generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=False,device=args.device)\n",
    "\n",
    "\n",
    "                if (step + 1) % (10*args.gradient_accumulation_steps) == 0:\n",
    "                    results = evaluate(args, model, valid_dataset, ignore_index, global_step)\n",
    "                    # for key, value in results.items():\n",
    "                    #     writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                    print('After', global_step+1,'updates: ', end='\\n\\n')\n",
    "                    generate_sample(valid_dataset, tokenizer, model, num=2, eval_step=True,device=args.device)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(args, model, eval_dataset, ignore_index, global_step=None):\n",
    "        \"\"\" Returns perplexity score on validation dataset.\n",
    "            Args:\n",
    "                args: dict that contains all the necessary information passed by user while training\n",
    "                model: finetuned gpt/gpt2 model\n",
    "                eval_dataset: GPT21024Dataset object for validation data\n",
    "                global_step: no. of times gradients have backpropagated\n",
    "                ignore_index: token not considered in loss calculation\n",
    "        \"\"\"\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.mkdir(args.output_dir)\n",
    "        eval_output_dir = args.output_dir\n",
    "\n",
    "        results = {}\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size)\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = torch.tensor(batch['article']).to(args.device), torch.tensor(batch['article']).to(args.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(inputs)[0]\n",
    "                idx = batch['sum_idx'].item() # index of separator token\n",
    "                # only consider loss on reference summary just like seq2seq models\n",
    "                shift_logits = logits[..., idx:-1, :].contiguous()\n",
    "                shift_labels = labels[..., idx+1:].contiguous()\n",
    "                lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                eval_loss += lm_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "        result = {\n",
    "            \"perplexity\": perplexity\n",
    "        }\n",
    "        print(\"perplexity:\", perplexity.item())\n",
    "\n",
    "        if global_step:\n",
    "            output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "            with open(output_eval_file, \"a\") as f:\n",
    "                for key in sorted(result.keys()):\n",
    "                    f.write('\\n\\n')\n",
    "                    f.write(\"time = %s, %s = %s, step = %s\\n\" % (datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), key, str(result[key]), str(global_step)))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-characteristic",
   "metadata": {},
   "source": [
    "# 3. training loop\n",
    "# all parameters are stored in argsDict {}\n",
    "# model is saved at the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-harassment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "argsDict = { 'lr': 5e-5, 'seed': 42, 'n_gpu': 1, 'gradient_accumulation_steps': 32, 'batch_size': 1,\n",
    "        'num_workers': 4, 'device': torch.device('cuda'), 'num_train_epochs': 1, 'output_dir': 'output',\n",
    "        'model_dir': 'weights', 'fp16': True, 'fp16_opt_level': 'O0', 'max_grad_norm': 1.0,\n",
    "        'root_dir': 'CNN/gpt2_1024_data', 'ids_file': 'CNN/ids.json'\n",
    "       }\n",
    "args = SimpleNamespace(**argsDict)\n",
    "\n",
    "\n",
    "train_data = GPT21024Dataset('CNN/gpt2_1024_data','CNN/ids.json',mode='train',length=3000) #training on only 3000 datasets\n",
    "valid_data = GPT21024Dataset('CNN/gpt2_1024_data','CNN/ids.json',mode='valid',length=500)  #validation on only 500 datasets\n",
    "tokenizer = add_special_tokens()\n",
    "ignore_idx = tokenizer.pad_token_id\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(args.device)\n",
    "\n",
    "start = time.time()\n",
    "train(args, model, tokenizer, train_data, valid_data, ignore_idx)\n",
    "print('total time: ', (time.time()-start)/60, \" minutes\", end='\\n\\n')\n",
    "\n",
    "print('Saving trained model...')\n",
    "model_file = os.path.join('model_{}_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(args.fp16_opt_level,3000,args.num_train_epochs))\n",
    "config_file = os.path.join('config_{}_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(args.fp16_opt_level,3000,args.num_train_epochs))\n",
    "torch.save(model.state_dict(), model_file)\n",
    "model.config.to_json_file(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-interview",
   "metadata": {},
   "source": [
    "# ############################################################\n",
    "# 4. play with model, \n",
    "# can jump to this step if training is already done\n",
    "# ############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "understood-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "argsDictTest = { 'seed': 42,\n",
    "        'num_workers': 4, 'device': torch.device('cuda'), 'num_train_epochs': 1, 'output_dir': 'output',\n",
    "        'model_dir': 'weights',\n",
    "        'root_dir': 'CNN/gpt2_1024_data', 'ids_file': 'CNN/ids.json'\n",
    "       }\n",
    "argsTest = SimpleNamespace(**argsDictTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vital-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "# using the same validation and training data as during training\n",
    "tokenizer = add_special_tokens()\n",
    "# train_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='train',length=3000)\n",
    "# valid_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='valid',length=500)\n",
    "test_data = GPT21024Dataset(argsTest.root_dir,argsTest.ids_file,mode='test',length=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "married-hughes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = \"weights/model_O0_data3000_trained_after_5_epochs_only_sum_loss_ignr_pad.bin\"\n",
    "config_file = \"weights/config_O0_data3000_trained_after_5_epochs_only_sum_loss_ignr_pad.json\"\n",
    "\n",
    "config = GPT2Config.from_json_file(config_file)\n",
    "model = GPT2LMHeadModel(config)\n",
    "state_dict = torch.load(model_file)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(argsTest.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "civilian-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-649ccaf0d589>:160: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(length):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a65a674f3bf41348473310f71ef6265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "-LRB- CNN -RRB- -- The number of birth defects in China are on the rise and the rate has nearly doubled in the past decade in Beijing and several provinces, a state-run newspaper reported Tuesday. The increase is mainly due to improved diagnostic techniques and monitoring capability, as well as women waiting until they are older to have children, China Daily reported, citing the Beijing municipal health bureau. Environmental pollution could also be a factor, the newspaper said, quoting Caijing magazine. The rate of birth defects in Beijing last year was 170 per 10,000 births, nearly twice the rate in 1997, when it was 90 birth defects per 10,000 births, China Daily reported, using figures from the health bureau. The most common birth defects in Beijing's Shunyi district are congenital heart disease, excessive fingers or toes, cleft lip or palate, and neural tube defects, a regional reproductive health officer told China Daily. A rise in birth defects was also seen in the provinces of Zhejiang, Hunan, Jiangxi, and Guangdong, China Daily reported, citing Caijing magazine.\n",
      "\n",
      "generated_summary\n",
      "\n",
      " The rate of birth defects in Beijing last year was 170 per 10,000 births , nearly twice the rate in 1997 . Environmental pollution could also be a factor , the newspaper says . The rate of birth defects in Beijing last year was 170 per 10,000 births , nearly twice the rate in 1997 . The most common birth defects in Beijing last year was congenital heart disease , excessive fingers or toes , cleft lip or palate . The rate of birth defects in Beijing last year was 170 per 10,\n",
      "\n",
      "actual_summary\n",
      "\n",
      "Birth defect rate nearly doubled in past decade in Beijing, state media says. Increase put down to better diagnostics and monitoring. Pollution and more older women having babies also seen as factors. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f019750178472fa9b2df9a398ab4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "Goma, Democratic Republic of Congo -LRB- CNN -RRB- -- The United Nations and Congolese army are sending additional soldiers to the troubled regional capital of Goma, a U.N. spokesman said Wednesday. `` Troops are being deployed at the town entrance, on the road that goes to Rutshuru, '' said Alexander Essome, MONUSCO regional spokesperson. MONUSCO, the U.N. peacekeeping mission in Democratic Republic of Congo, `` has been reinforced with extra troops in the area, '' Essome said. Rutshuru, 60 kilometers -LRB- 37 miles -RRB- north of Goma, was taken by M23 rebels on July 1, but they retreated within days and the Congolese regular army moved back into the town Wednesday. The Congolese army is dispatching a U.S. trained elite battalion to Goma, where most shops were closed Wednesday because of threats from M23 to take Goma. The M23, linked to Laurent Nkunda's CNDP rebel group, still controls the town of Bunagana. Goma is in on the border with Rwanda on Lake Kivu.\n",
      "\n",
      "generated_summary\n",
      "\n",
      " The U.N. peacekeeping mission is in Democratic Republic of Congo . The Congolese army is dispatching a U.S. trained elite battalion to Goma . The U.N. peacekeeping mission is in Democratic Republic of Congo . The Congolese army is dispatching a U.S. trained elite battalion to Goma . The U.N. peacekeeping mission is in Democratic Republic of Congo . The Congolese army is dispatching a U.S. trained elite\n",
      "\n",
      "actual_summary\n",
      "\n",
      "U.N. spokesman : Extra troops reinforcing regional capital on border with Rwanda. M23 rebels have threatened to take over the town. One unit is Congolese elite battalion trained by U.S. troops. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(test_data, tokenizer, model, num=2, length=100, temperature=1, top_k=10, top_p=0.5, device=argsTest.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-asthma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nonprofit-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-649ccaf0d589>:186: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_token_probs = F.softmax(next_token_logits)\n",
      "<ipython-input-1-649ccaf0d589>:190: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(length-1):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17848692df304a3d99b6ebe82d5ca577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-649ccaf0d589>:197: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_token_probs = F.softmax(next_token_logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "-LRB- CNN -RRB- -- The number of birth defects in China are on the rise and the rate has nearly doubled in the past decade in Beijing and several provinces, a state-run newspaper reported Tuesday. The increase is mainly due to improved diagnostic techniques and monitoring capability, as well as women waiting until they are older to have children, China Daily reported, citing the Beijing municipal health bureau. Environmental pollution could also be a factor, the newspaper said, quoting Caijing magazine. The rate of birth defects in Beijing last year was 170 per 10,000 births, nearly twice the rate in 1997, when it was 90 birth defects per 10,000 births, China Daily reported, using figures from the health bureau. The most common birth defects in Beijing's Shunyi district are congenital heart disease, excessive fingers or toes, cleft lip or palate, and neural tube defects, a regional reproductive health officer told China Daily. A rise in birth defects was also seen in the provinces of Zhejiang, Hunan, Jiangxi, and Guangdong, China Daily reported, citing Caijing magazine\n",
      "\n",
      "actual_summary\n",
      "\n",
      "Birth defect rate nearly doubled in past decade in Beijing, state media says. Increase put down to better diagnostics and monitoring. Pollution and more older women having babies also seen as factors. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 5.411237690402984e-16.\n",
      "\n",
      " The Daily could not verify the report of the defect rate . The birth defect rate in China has doubled in the past decade . The pollution could also be a factor , the newspaper says . The birth defect rate in China has doubled in the past decade . The birth defect rate in China has doubled in the past decade . The birth defect rate in China has doubled in the past decade . The birth defect rate in China has doubled in the past decade . The birth defect rate in China has doubled in the past\n",
      "\n",
      "generated_summary-2 and Score is 3.405800036029089e-17.\n",
      "\n",
      " China pollution of air independently of number . birth health-run says and of of has Beijing is of the recent past . , Beijing rate of birth defect in recent . a report said birth defect rate has doubled in Beijing and several provinces . Environmental pollution could Environmental pollution could also be a factor , the newspaper says . , China pollution of air independently of number . birth health-run says China pollution of air independently of number . birth health-run says China pollution of air independently of number . birth health-\n",
      "\n",
      "generated_summary-3 and Score is 4.1732355332672465e-19.\n",
      "\n",
      " Environmental rate is pollution confirmatory source of a state- in Beijing rate rate rises is increased rate on birth highest in decades in Environmental birth defect rate defects is cause years state rise in , China rate in recent has doubled is due to recent The five , A pollution-related has Beijing is the most recent said to The Environmental Daily rate of has doubled is of China recent birth year , Environmental rate of birth has doubled is tripled in recent years year , A pollution-related independent says is is cause past last\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5912fe7faca64d099e16bfa2dea1ab6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "Goma, Democratic Republic of Congo -LRB- CNN -RRB- -- The United Nations and Congolese army are sending additional soldiers to the troubled regional capital of Goma, a U.N. spokesman said Wednesday. `` Troops are being deployed at the town entrance, on the road that goes to Rutshuru, '' said Alexander Essome, MONUSCO regional spokesperson. MONUSCO, the U.N. peacekeeping mission in Democratic Republic of Congo, `` has been reinforced with extra troops in the area, '' Essome said. Rutshuru, 60 kilometers -LRB- 37 miles -RRB- north of Goma, was taken by M23 rebels on July 1, but they retreated within days and the Congolese regular army moved back into the town Wednesday. The Congolese army is dispatching a U.S. trained elite battalion to Goma, where most shops were closed Wednesday because of threats from M23 to take Goma. The M23, linked to Laurent Nkunda's CNDP rebel group, still controls the town of Bunagana. Goma is in on the border with Rwanda on Lake Kivu\n",
      "\n",
      "actual_summary\n",
      "\n",
      "U.N. spokesman : Extra troops reinforcing regional capital on border with Rwanda. M23 rebels have threatened to take over the town. One unit is Congolese elite battalion trained by U.S. troops. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 1.4444127588875838e-14.\n",
      "\n",
      " The U.N. peacekeeping mission in Democratic Republic of Congo is been reinforced with additional troops . TheomaolNkunda rebels thest.ian23 rebels in Goma . The U.N. peacekeeping mission in Democratic Republic of Congo is being reinforced with additional troops . The U.N. peacekeeping mission in Democratic Republic of Congo is being reinforced with additional troops . The U.N. peacekeeping mission in Democratic Republic of Congo is being reinforced with additional troops . The U.\n",
      "\n",
      "generated_summary-2 and Score is 7.911182907027942e-16.\n",
      "\n",
      " U Congolese troops are also. is been deployed to G has reinforced with extra extra troops . G Cong is is inoma on 1 Uol M troops to the region . The U.N. peacekeeping mission in Democratic Republic of Congo is has been reinforced with additional troops . The Congolese army is dispatching a U.S. trained elite battalion to Goma . Goma is in on the border with Rwanda on Lake Kivu . U.ol M troops G. is\n",
      "\n",
      "generated_summary-3 and Score is 4.1002302701915235e-16.\n",
      "\n",
      " M.23 rebels soldiers and U deployed has deploying troops . the , also strengthened by troops , in The U. in G on July are Cong ofice army . the town is in Congolese territory . has been is is in Democratic Republic has been also in . extra troops are Goma 'S military is also missions is Congo 's.- has also been . the . , The U.N. peacekeeping mission in . Congo is in has also been CongN U. Uoma has\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_beam_sample(test_data, tokenizer, model, num=2, length=100, beam_size=3, device=argsTest.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-premiere",
   "metadata": {},
   "source": [
    "## Download An Article Given A Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "present-maine",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googlesearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-372d253ce292>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgooglesearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Get url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'googlesearch'"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "def sentences_from_query(query):\n",
    "    # Get url\n",
    "    if query.startswith(\"http\"):\n",
    "        url = query\n",
    "    else:\n",
    "        url = search(query, num_results=1)[0]\n",
    "    print(url)\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    # Get text from all <p> tags.\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Get the text from each of the \"p\" tags and strip surrounding whitespace.\n",
    "    p_tags_text = \" \".join([tag.get_text().strip() for tag in p_tags])\n",
    "    return p_tags_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = sentences_from_query(\"neural embedding\")\n",
    "article = tokenizer.encode(article)[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = sample_seq(model, article, 50, args.device, temperature=1, top_k=10, top_p=0.5)\n",
    "generated_text = generated_text[0, len(article):].tolist()\n",
    "text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "text = tokenizer.convert_tokens_to_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Article: \\n\")\n",
    "print(tokenizer.decode(article))\n",
    "print(\"------------------------------------------------------------ \\n\")\n",
    "print(\"Generated Summary: \\n\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
